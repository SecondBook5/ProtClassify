{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aflVNG4w4lzj",
        "outputId": "990d7a52-ea5e-4a41-b0ca-f5f6ba83ed6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into ProtClassify folder\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/ProtClassify')\n",
        "print(\"PWD:\", os.getcwd())\n",
        "# Should list all your notebooks, data/, combined_with_pfam.parquet, etc.\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvghxEWR4lmS",
        "outputId": "5485e1ac-ebf3-437d-e98f-9b5d4e07fc88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PWD: /content/drive/MyDrive/ProtClassify\n",
            "total 63623\n",
            "-rw------- 1 root root   213993 Apr 17 23:27 Analysis_Preprocess_Assignment8.ipynb\n",
            "-rw------- 1 root root  3800496 Apr 21 21:16 combined_with_pfam.parquet\n",
            "-rw------- 1 root root   190991 Apr 24 15:58 Competition_Tuning_Assignment12.ipynb\n",
            "drwx------ 2 root root     4096 Apr 24 17:08 data\n",
            "drwx------ 2 root root     4096 Apr 24 17:08 docs\n",
            "drwx------ 2 root root     4096 Apr 24 17:08 ensemble_output\n",
            "drwx------ 2 root root     4096 Apr 24 17:08 external_tools\n",
            "-rw------- 1 root root   835872 Apr 21 06:38 feature_scores_combined.csv\n",
            "drwx------ 2 root root     4096 Apr 24 17:08 feature_selector\n",
            "-rw------- 1 root root      734 Apr 18 08:53 feature_selector_env.yaml\n",
            "-rw------- 1 root root   619649 Mar  9 21:57 metadata_org.csv\n",
            "-rw------- 1 root root  7191855 Apr  1 03:51 metadata_org_w_features.csv\n",
            "-rw------- 1 root root  1912656 Apr 22 04:09 Model_Eval_Assignment10.ipynb\n",
            "-rw------- 1 root root   716874 Apr 20 06:59 Model_Eval_Assignment9.ipynb\n",
            "-rw------- 1 root root     1194 Apr 22 03:46 model_results.csv\n",
            "-rw------- 1 root root  1409276 Apr 21 04:48 peptide_descriptors.csv\n",
            "-rw------- 1 root root 13736218 Apr 21 04:49 protlearn_features.csv\n",
            "-rw------- 1 root root   700544 Apr 24 15:16 pt5_eval.npy\n",
            "-rw------- 1 root root  2789504 Apr 24 15:16 pt5_train.npy\n",
            "drwx------ 2 root root     4096 Apr 24 17:08 __pycache__\n",
            "-rw------- 1 root root       32 Apr 23 08:05 README.md\n",
            "-rw------- 1 root root  1610612 Apr 24 04:46 test.csv\n",
            "-rw------- 1 root root  1769367 Apr 14 01:17 testing_data_w_features.csv\n",
            "-rw------- 1 root root  6539687 Apr 24 04:46 training.csv\n",
            "-rw------- 1 root root     1931 Apr 17 23:37 vhse_predictions.csv\n",
            "-rw------- 1 root root 21073762 Apr 21 04:49 X_all_features.csv\n",
            "-rw------- 1 root root     4881 Apr 21 04:49 y_all_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "kWJG60RK8UMi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "train_csv = \"metadata_org_w_features.csv\"\n",
        "test_csv = \"testing_data_w_features.csv\"\n",
        "model_name = \"Rostlab/prot_bert_bfd\"\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "OKqB9JXH6InW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "model = BertModel.from_pretrained(model_name).to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laRAStpwKv2J",
        "outputId": "c0f16e18-c3ab-4ded-d4f9-270ef8139c6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function (adds spaces between residues)\n",
        "def preprocess(seqs):\n",
        "    return [\" \".join(list(s)) for s in seqs]"
      ],
      "metadata": {
        "id": "SLu77_n3K0qG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding function\n",
        "def embed_protbert(seqs, batch_size=8):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(seqs), batch_size):\n",
        "        batch_seqs = preprocess(seqs[i:i+batch_size])\n",
        "        tokens = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**tokens)\n",
        "            last_hidden = output.last_hidden_state  # (B, L, 1024)\n",
        "\n",
        "        mask = tokens[\"attention_mask\"].unsqueeze(-1)\n",
        "        pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "        all_embeddings.append(pooled.cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_embeddings)"
      ],
      "metadata": {
        "id": "s_LS2iOQK6iw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and embed training set\n",
        "df_train = pd.read_csv(train_csv)\n",
        "train_seqs = df_train[\"CleanSequence\"].drop_duplicates().tolist()\n",
        "print(f\"Embedding {len(train_seqs)} training sequences with ProtBERT...\")\n",
        "protbert_train = embed_protbert(train_seqs, batch_size=batch_size)\n",
        "np.save(\"protbert_train.npy\", protbert_train)\n",
        "print(\"Saved protbert_train.npy:\", protbert_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nDqrpjZK9Su",
        "outputId": "1e44ef30-1849-4cc4-ea34-169703959bf6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding 679 training sequences with ProtBERT...\n",
            "Saved protbert_train.npy: (679, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and embed test set\n",
        "df_test = pd.read_csv(test_csv)\n",
        "test_seqs = df_test[\"CleanSequence\"].drop_duplicates().tolist()\n",
        "print(f\"Embedding {len(test_seqs)} test sequences with ProtBERT...\")\n",
        "protbert_eval = embed_protbert(test_seqs, batch_size=batch_size)\n",
        "np.save(\"protbert_eval.npy\", protbert_eval)\n",
        "print(\"Saved protbert_eval.npy:\", protbert_eval.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwDeEIj6K-Rm",
        "outputId": "a5a25a69-f7df-430b-bdba-49de9be452d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding 170 test sequences with ProtBERT...\n",
            "Saved protbert_eval.npy: (170, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "# Load the embeddings\n",
        "protbert_train = np.load(\"protbert_train.npy\")\n",
        "protbert_eval = np.load(\"protbert_eval.npy\")\n",
        "\n",
        "# Print shapes\n",
        "print(\"Train shape:\", protbert_train.shape)\n",
        "print(\"Eval shape: \", protbert_eval.shape)\n",
        "\n",
        "# Check uniqueness among first few rows\n",
        "train_unique = len(set(map(tuple, protbert_train[:5])))\n",
        "eval_unique = len(set(map(tuple, protbert_eval[:5])))\n",
        "print(\"Unique rows (train, first 5):\", train_unique)\n",
        "print(\"Unique rows (eval, first 5): \", eval_unique)\n",
        "\n",
        "# Print first 5Ã—10 slice from training embeddings\n",
        "print(\"\\nFirst 5 training sequences (first 10 dims):\")\n",
        "print(protbert_train[:5, :10])\n",
        "\n",
        "# Print L2 distance between row 0 and 1 to ensure non-trivial embeddings\n",
        "print(\"\\nTrain distance row 0â€“1:\", norm(protbert_train[0] - protbert_train[1]))\n",
        "print(\"Eval distance row 0â€“1: \", norm(protbert_eval[0] - protbert_eval[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo10_3OdPfFh",
        "outputId": "f9e1f9fc-1efc-42cc-8719-80ccc7afa772"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (679, 1024)\n",
            "Eval shape:  (170, 1024)\n",
            "Unique rows (train, first 5): 5\n",
            "Unique rows (eval, first 5):  5\n",
            "\n",
            "First 5 training sequences (first 10 dims):\n",
            "[[-0.00308354  0.01258058  0.00362734 -0.01812596  0.01308226  0.02219505\n",
            "  -0.04585007 -0.04406652 -0.0002828  -0.01508848]\n",
            " [ 0.01189695  0.00605286  0.00143205  0.01612679  0.0310211   0.00919091\n",
            "  -0.0135506  -0.03638414 -0.01489322 -0.0058881 ]\n",
            " [ 0.02996278  0.02738642 -0.01442665  0.00224002  0.00327666 -0.01447352\n",
            "   0.00874932 -0.0274386  -0.01067536  0.03063629]\n",
            " [ 0.01004064  0.00632988 -0.02497391  0.00205142  0.0122145   0.00715791\n",
            "  -0.01774822 -0.00409395  0.00128063  0.01881311]\n",
            " [ 0.00153544 -0.00827346 -0.00969256 -0.01578032  0.0480093   0.02024044\n",
            "   0.01480491 -0.03880088  0.00665801 -0.00490303]]\n",
            "\n",
            "Train distance row 0â€“1: 1.166804\n",
            "Eval distance row 0â€“1:  1.51118\n"
          ]
        }
      ]
    }
  ]
}